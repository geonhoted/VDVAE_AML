{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# model_hps.py\n",
        "\n",
        "VDVAE의 encoder, decoder 내부 layer 수 및 이미지 설정 등 모델 구조에 관련된 하이퍼파라미터를 여기서 관리합니다."
      ],
      "metadata": {
        "id": "bgyz6Akontiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we manage parameters for model structure here.\n",
        "block_str = \"32x11,32d2,16x6,16d2,8x6,8d2,4x3,4d4,1x3\"    # encoder 구조\n",
        "custom_width_str = \"\"                                     # 해상도별 채널 수 조정 (16:64,8:64)\n",
        "res_list = [(1, None), (4, 1), (8, 4), (16, 8), (32, 16)] # decoder 구조\n",
        "\n",
        "image_size = 32\n",
        "image_channels = 3\n",
        "base_width = 384\n",
        "bottleneck_multiple = 0.25\n",
        "zdim = 16\n",
        "num_mixtures = 10\n",
        "n_blocks = 3"
      ],
      "metadata": {
        "id": "wI5b0eOwn3oY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hps.py\n",
        "\n",
        "모델의 학습 관련 및 저장소, 데이터 루트 등의 하이퍼파라미터는 모두 여기 모아서 관리합니다."
      ],
      "metadata": {
        "id": "0PvpJUvWnlf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l6X9voPCj44H"
      },
      "outputs": [],
      "source": [
        "HPARAMS_REGISTRY = {}\n",
        "\n",
        "class Hyperparams(dict):\n",
        "    def __getattr__(self, attr):\n",
        "        try:\n",
        "            return self[attr]\n",
        "        except KeyError:\n",
        "            return None\n",
        "\n",
        "    def __setattr__(self, attr, value):\n",
        "        self[attr] = value\n",
        "\n",
        "\n",
        "# We only use CIFAR-10 dataset\n",
        "cifar10 = Hyperparams()\n",
        "cifar10.dataset = 'cifar10'\n",
        "cifar10.lr = 0.0002\n",
        "cifar10.wd = 0.01\n",
        "cifar10.n_batch = 32\n",
        "cifar10.ema_rate =  0.9998\n",
        "cifar10.warmup_iters = 100\n",
        "cifar10.skip_threshold = 400.0\n",
        "cifar10.max_iters = 1563        # training ends up based on which is longer between max_iters & epoch.\n",
        "cifar10.num_epochs = 10\n",
        "HPARAMS_REGISTRY['cifar10'] = cifar10\n",
        "\n",
        "\n",
        "def parse_args_and_update_hparams(H, parser, s=None):\n",
        "    args = parser.parse_args(s)\n",
        "    valid_args = set(vars(args).keys())\n",
        "\n",
        "    hps = HPARAMS_REGISTRY['cifar10']\n",
        "    for k in hps:\n",
        "        if k not in valid_args:\n",
        "            raise ValueError(f\"{k} not in default args\")\n",
        "    parser.set_defaults(**hps)\n",
        "    args = parser.parse_args(s)\n",
        "    H.update(vars(args))\n",
        "\n",
        "\n",
        "# we manage all parameters here except for model structure.\n",
        "def add_vae_arguments(parser):\n",
        "    parser.add_argument('--seed', type=int, default=0)\n",
        "    parser.add_argument('--port', type=int, default=29500)\n",
        "    parser.add_argument('--save_dir', type=str, default='./saved_models')\n",
        "    parser.add_argument('--data_root', type=str, default='../content')\n",
        "\n",
        "    parser.add_argument('--desc', type=str, default='test')\n",
        "    parser.add_argument('--restore_path', type=str, default=None,\n",
        "                        help=\"checkpoint prefix를 지정하면 그 지점부터 학습을 복원!\")        # default = './saved_models/test/latest'\n",
        "    parser.add_argument('--restore_ema_path', type=str, default=None)                        # default='./saved_models/test/latest'\n",
        "    parser.add_argument('--restore_log_path', type=str, default=None)                        # default='./saved_models/test/latest-log.jsonl'\n",
        "    parser.add_argument('--restore_optimizer_path', type=str, default=None)                  # default='./saved_models/test/latest-opt.th'\n",
        "    parser.add_argument('--dataset', type=str, default='cifar10')\n",
        "\n",
        "    parser.add_argument('--ema_rate', type=float, default=0.999)\n",
        "\n",
        "    parser.add_argument('--test_eval', action=\"store_true\")\n",
        "    parser.add_argument('--warmup_iters', type=float, default=0)\n",
        "\n",
        "    parser.add_argument('--grad_clip', type=float, default=200.0)\n",
        "    parser.add_argument('--skip_threshold', type=float, default=400.0)\n",
        "    parser.add_argument('--lr', type=float, default=0.00015)\n",
        "    parser.add_argument('--lr_prior', type=float, default=0.00015)\n",
        "    parser.add_argument('--wd', type=float, default=0.0)\n",
        "    parser.add_argument('--wd_prior', type=float, default=0.0)\n",
        "    parser.add_argument('--num_epochs', type=int, default=10)                 # 10000 (maximum)\n",
        "    parser.add_argument('--n_batch', type=int, default=32)\n",
        "    parser.add_argument('--adam_beta1', type=float, default=0.9)\n",
        "    parser.add_argument('--adam_beta2', type=float, default=0.9)\n",
        "\n",
        "    parser.add_argument('--temperature', type=float, default=1.0)\n",
        "\n",
        "    parser.add_argument('--iters_per_ckpt', type=int, default=25000)\n",
        "    parser.add_argument('--iters_per_print', type=int, default=1000)\n",
        "    parser.add_argument('--iters_per_save', type=int, default=1500)          # 10000\n",
        "    parser.add_argument('--iters_per_images', type=int, default=10000)\n",
        "    parser.add_argument('--epochs_per_eval', type=int, default=10)            # number of epoch\n",
        "    parser.add_argument('--epochs_per_probe', type=int, default=None)\n",
        "    parser.add_argument('--epochs_per_eval_save', type=int, default=20)\n",
        "    parser.add_argument('--num_images_visualize', type=int, default=8)\n",
        "    parser.add_argument('--num_variables_visualize', type=int, default=6)\n",
        "    parser.add_argument('--num_temperatures_visualize', type=int, default=3)\n",
        "    parser.add_argument('--max_iters', type=int, default=3125)                # number of maximum iterations\n",
        "    return parser"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data.py"
      ],
      "metadata": {
        "id": "KZV6kwt5qnut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def set_up_data(H):\n",
        "    shift_loss = -127.5\n",
        "    scale_loss = 1. / 127.5\n",
        "    if H.dataset == 'cifar10':\n",
        "        (trX, _), (vaX, _), (teX, _) = load_cifar10_data(H.data_root, one_hot=False)\n",
        "        H.image_size = 32\n",
        "        H.image_channels = 3\n",
        "        shift = -120.63838\n",
        "        scale = 1. / 64.16736\n",
        "    else:\n",
        "        raise ValueError('unknown dataset: ', H.dataset)\n",
        "\n",
        "    if H.test_eval:\n",
        "        print('DOING TEST')\n",
        "        eval_dataset = teX\n",
        "    else:\n",
        "        eval_dataset = vaX\n",
        "\n",
        "    # Reshape shift, scale, shift_loss, and scale_loss for broadcasting\n",
        "    shift = torch.tensor([shift]).cuda().view(1, 1, 1, 1)\n",
        "    scale = torch.tensor([scale]).cuda().view(1, 1, 1, 1)\n",
        "    shift_loss = torch.tensor([shift_loss]).cuda().view(1, 1, 1, 1)\n",
        "    scale_loss = torch.tensor([scale_loss]).cuda().view(1, 1, 1, 1)\n",
        "\n",
        "\n",
        "    train_data = TensorDataset(torch.as_tensor(trX))\n",
        "    valid_data = TensorDataset(torch.as_tensor(eval_dataset))\n",
        "\n",
        "    def preprocess_func(x):\n",
        "        nonlocal shift\n",
        "        nonlocal scale\n",
        "        nonlocal shift_loss\n",
        "        nonlocal scale_loss\n",
        "        inp = x[0].cuda(non_blocking=True).float()\n",
        "        out = inp.clone()\n",
        "        inp.add_(shift).mul_(scale)\n",
        "        out.add_(shift_loss).mul_(scale_loss)\n",
        "        return inp, out\n",
        "\n",
        "    return H, train_data, valid_data, preprocess_func\n",
        "\n",
        "\n",
        "def unpickle_cifar10(file):\n",
        "    fo = open(file, 'rb')\n",
        "    data = pickle.load(fo, encoding='bytes')\n",
        "    fo.close()\n",
        "    data = dict(zip([k.decode() for k in data.keys()], data.values()))\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_cifar10_data(data_root, one_hot=True):\n",
        "    root = os.path.join(data_root, 'cifar-10-batches-py')\n",
        "\n",
        "    # load training batches\n",
        "    data_list, label_list = [], []\n",
        "    for i in range(1, 6):\n",
        "        batch_path = os.path.join(root, f'data_batch_{i}')\n",
        "        batch = unpickle_cifar10(batch_path)\n",
        "        data_list.append(batch['data'])\n",
        "        label_list.append(batch['labels'])\n",
        "    trX = np.concatenate(data_list, axis=0).astype(np.float32)               # (50000, 3072)\n",
        "    trY = np.concatenate(label_list, axis=0).astype(np.int64)                # (50000,)\n",
        "\n",
        "    # load test batches\n",
        "    test_batch = unpickle_cifar10(os.path.join(root, 'test_batch'))\n",
        "    teX = np.array(test_batch['data'], dtype=np.uint8).astype(np.float32)    # (10000, 3072)\n",
        "    teY = np.array(test_batch['labels'], dtype=np.int64)                     # (10000,)\n",
        "\n",
        "    trX = trX.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "    teX = teX.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "    trX, vaX, trY, vaY = train_test_split(trX, trY, test_size=5000, random_state=11172018)\n",
        "\n",
        "    if one_hot:\n",
        "        trY = np.eye(10, dtype=np.float32)[trY]\n",
        "        vaY = np.eye(10, dtype=np.float32)[vaY]\n",
        "        teY = np.eye(10, dtype=np.float32)[teY]\n",
        "    else:\n",
        "        trY = np.reshape(trY, [-1, 1])\n",
        "        vaY = np.reshape(vaY, [-1, 1])\n",
        "        teY = np.reshape(teY, [-1, 1])\n",
        "    return (trX, trY), (vaX, vaY), (teX, teY)"
      ],
      "metadata": {
        "id": "yBl6pTg8qorY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils.py"
      ],
      "metadata": {
        "id": "MeEYQgGiqq1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_C90XuuquLq",
        "outputId": "dccf6887-01ae-4cf0-d14b-5b00c76deeb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4441849 sha256=831424c6b9e310c22847972a4ccb19f31693f21e3c2ed19d9a66bc344675c00f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "import os\n",
        "import json\n",
        "import socket\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel.distributed import DistributedDataParallel\n",
        "from torch.optim import AdamW\n",
        "from collections import defaultdict\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def parse_layer_string(s):\n",
        "    layers = []\n",
        "    for ss in s.split(','):\n",
        "        if 'x' in ss:\n",
        "            res, num = ss.split('x')\n",
        "            count = int(num)\n",
        "            layers += [(int(res), None) for _ in range(count)]\n",
        "        elif 'm' in ss:\n",
        "            res, mixin = [int(a) for a in ss.split('m')]\n",
        "            layers.append((res, mixin))\n",
        "        elif 'd' in ss:\n",
        "            res, down_rate = [int(a) for a in ss.split('d')]\n",
        "            layers.append((res, down_rate))\n",
        "        else:\n",
        "            res = int(ss)\n",
        "            layers.append((res, None))\n",
        "    return layers\n",
        "\n",
        "\n",
        "def pad_channels(t, width):\n",
        "    d1, d2, d3, d4 = t.shape\n",
        "    empty = torch.zeros(d1, width, d3, d4, device=t.device)\n",
        "    empty[:, :d2, :, :] = t\n",
        "    return empty\n",
        "\n",
        "\n",
        "def get_width_settings(width, s):\n",
        "    mapping = defaultdict(lambda: width)\n",
        "    if s:\n",
        "        s = s.split(',')\n",
        "        for ss in s:\n",
        "            k, v = ss.split(':')\n",
        "            mapping[int(k)] = int(v)\n",
        "    return mapping\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def gaussian_analytical_kl(mu1, mu2, logsigma1, logsigma2):\n",
        "    return -0.5 + logsigma2 - logsigma1 + 0.5 * (logsigma1.exp() ** 2 + (mu1 - mu2) ** 2) / (logsigma2.exp() ** 2)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def draw_gaussian_diag_samples(mu, logsigma):\n",
        "    eps = torch.empty_like(mu).normal_(0., 1.)\n",
        "    return torch.exp(logsigma) * eps + mu\n",
        "\n",
        "\n",
        "def discretized_mix_logistic_loss(x, l, low_bit=False):\n",
        "    \"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"\n",
        "    # Adapted from https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/nn.py\n",
        "    xs = [s for s in x.shape]  # true image (i.e. labels) to regress to, e.g. (B,32,32,3)\n",
        "    ls = [s for s in l.shape]  # predicted distribution, e.g. (B,32,32,100)\n",
        "    nr_mix = int(ls[-1] / 10)  # here and below: unpacking the params of the mixture of logistics\n",
        "    logit_probs = l[:, :, :, :nr_mix]\n",
        "    l = torch.reshape(l[:, :, :, nr_mix:], xs + [nr_mix * 3])\n",
        "    means = l[:, :, :, :, :nr_mix]\n",
        "    log_scales = const_max(l[:, :, :, :, nr_mix:2 * nr_mix], -7.)\n",
        "    coeffs = torch.tanh(l[:, :, :, :, 2 * nr_mix:3 * nr_mix])\n",
        "    x = torch.reshape(x, xs + [1]) + torch.zeros(xs + [nr_mix]).to(x.device)  # here and below: getting the means and adjusting them based on preceding sub-pixels\n",
        "    m2 = torch.reshape(means[:, :, :, 1, :] + coeffs[:, :, :, 0, :] * x[:, :, :, 0, :], [xs[0], xs[1], xs[2], 1, nr_mix])\n",
        "    m3 = torch.reshape(means[:, :, :, 2, :] + coeffs[:, :, :, 1, :] * x[:, :, :, 0, :] + coeffs[:, :, :, 2, :] * x[:, :, :, 1, :], [xs[0], xs[1], xs[2], 1, nr_mix])\n",
        "    means = torch.cat([torch.reshape(means[:, :, :, 0, :], [xs[0], xs[1], xs[2], 1, nr_mix]), m2, m3], dim=3)\n",
        "    centered_x = x - means\n",
        "    inv_stdv = torch.exp(-log_scales)\n",
        "    if low_bit:\n",
        "        plus_in = inv_stdv * (centered_x + 1. / 31.)\n",
        "        cdf_plus = torch.sigmoid(plus_in)\n",
        "        min_in = inv_stdv * (centered_x - 1. / 31.)\n",
        "    else:\n",
        "        plus_in = inv_stdv * (centered_x + 1. / 255.)\n",
        "        cdf_plus = torch.sigmoid(plus_in)\n",
        "        min_in = inv_stdv * (centered_x - 1. / 255.)\n",
        "    cdf_min = torch.sigmoid(min_in)\n",
        "    log_cdf_plus = plus_in - F.softplus(plus_in)  # log probability for edge case of 0 (before scaling)\n",
        "    log_one_minus_cdf_min = -F.softplus(min_in)  # log probability for edge case of 255 (before scaling)\n",
        "    cdf_delta = cdf_plus - cdf_min  # probability for all other cases\n",
        "    mid_in = inv_stdv * centered_x\n",
        "    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)  # log probability in the center of the bin, to be used in extreme cases (not actually used in our code)\n",
        "\n",
        "    # now select the right output: left edge case, right edge case, normal case, extremely low prob case (doesn't actually happen for us)\n",
        "\n",
        "    # this is what we are really doing, but using the robust version below for extreme cases in other applications and to avoid NaN issue with tf.select()\n",
        "    # log_probs = tf.select(x < -0.999, log_cdf_plus, tf.select(x > 0.999, log_one_minus_cdf_min, tf.log(cdf_delta)))\n",
        "\n",
        "    # robust version, that still works if probabilities are below 1e-5 (which never happens in our code)\n",
        "    # tensorflow backpropagates through tf.select() by multiplying with zero instead of selecting: this requires use to use some ugly tricks to avoid potential NaNs\n",
        "    # the 1e-12 in tf.maximum(cdf_delta, 1e-12) is never actually used as output, it's purely there to get around the tf.select() gradient issue\n",
        "    if low_bit:\n",
        "        log_probs = torch.where(x < -0.999,\n",
        "                                log_cdf_plus,\n",
        "                                torch.where(x > 0.999,\n",
        "                                            log_one_minus_cdf_min,\n",
        "                                            torch.where(cdf_delta > 1e-5,\n",
        "                                                        torch.log(const_max(cdf_delta, 1e-12)),\n",
        "                                                        log_pdf_mid - np.log(15.5))))\n",
        "    else:\n",
        "        log_probs = torch.where(x < -0.999,\n",
        "                                log_cdf_plus,\n",
        "                                torch.where(x > 0.999,\n",
        "                                            log_one_minus_cdf_min,\n",
        "                                            torch.where(cdf_delta > 1e-5,\n",
        "                                                        torch.log(const_max(cdf_delta, 1e-12)),\n",
        "                                                        log_pdf_mid - np.log(127.5))))\n",
        "    log_probs = log_probs.sum(dim=3) + log_prob_from_logits(logit_probs)\n",
        "    mixture_probs = torch.logsumexp(log_probs, -1)\n",
        "    return -1. * mixture_probs.sum(dim=[1, 2]) / np.prod(xs[1:])\n",
        "\n",
        "\n",
        "def const_max(t, constant):\n",
        "    other = torch.ones_like(t) * constant\n",
        "    return torch.max(t, other)\n",
        "\n",
        "\n",
        "def const_min(t, constant):\n",
        "    other = torch.ones_like(t) * constant\n",
        "    return torch.min(t, other)\n",
        "\n",
        "\n",
        "def sample_from_discretized_mix_logistic(l, nr_mix):\n",
        "    ls = [s for s in l.shape]\n",
        "    xs = ls[:-1] + [3]\n",
        "    # unpack parameters\n",
        "    logit_probs = l[:, :, :, :nr_mix]\n",
        "    l = torch.reshape(l[:, :, :, nr_mix:], xs + [nr_mix * 3])\n",
        "    # sample mixture indicator from softmax\n",
        "    eps = torch.empty(logit_probs.shape, device=l.device).uniform_(1e-5, 1. - 1e-5)\n",
        "    amax = torch.argmax(logit_probs - torch.log(-torch.log(eps)), dim=3)\n",
        "    sel = F.one_hot(amax, num_classes=nr_mix).float()\n",
        "    sel = torch.reshape(sel, xs[:-1] + [1, nr_mix])\n",
        "    # select logistic parameters\n",
        "    means = (l[:, :, :, :, :nr_mix] * sel).sum(dim=4)\n",
        "    log_scales = const_max((l[:, :, :, :, nr_mix:nr_mix * 2] * sel).sum(dim=4), -7.)\n",
        "    coeffs = (torch.tanh(l[:, :, :, :, nr_mix * 2:nr_mix * 3]) * sel).sum(dim=4)\n",
        "    # sample from logistic & clip to interval\n",
        "    # we don't actually round to the nearest 8bit value when sampling\n",
        "    u = torch.empty(means.shape, device=means.device).uniform_(1e-5, 1. - 1e-5)\n",
        "    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n",
        "    x0 = const_min(const_max(x[:, :, :, 0], -1.), 1.)\n",
        "    x1 = const_min(const_max(x[:, :, :, 1] + coeffs[:, :, :, 0] * x0, -1.), 1.)\n",
        "    x2 = const_min(const_max(x[:, :, :, 2] + coeffs[:, :, :, 1] * x0 + coeffs[:, :, :, 2] * x1, -1.), 1.)\n",
        "    return torch.cat([torch.reshape(x0, xs[:-1] + [1]), torch.reshape(x1, xs[:-1] + [1]), torch.reshape(x2, xs[:-1] + [1])], dim=3)\n",
        "\n",
        "\n",
        "class DmolNet(nn.Module):\n",
        "    def __init__(self, width, num_mixtures, low_bit=False):\n",
        "        super().__init__()\n",
        "        self.width = width\n",
        "        self.num_mixtures = num_mixtures\n",
        "        self.low_bit = low_bit\n",
        "        self.out_conv = nn.Conv2d(width, num_mixtures * 10, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def nll(self, px_z, x):\n",
        "        return discretized_mix_logistic_loss(x=x, l=self.forward(px_z), low_bit=self.low_bit)\n",
        "\n",
        "    def forward(self, px_z):\n",
        "        if not isinstance(px_z, torch.Tensor):\n",
        "            if isinstance(px_z, np.ndarray):\n",
        "                px_z = torch.from_numpy(px_z).to(device=self.out_conv.weight.device, dtype=self.out_conv.weight.dtype).contiguous()\n",
        "        xhat = self.out_conv(px_z)\n",
        "        return xhat.permute(0, 2, 3, 1)\n",
        "\n",
        "    def sample(self, px_z):\n",
        "        im = sample_from_discretized_mix_logistic(self.forward(px_z), self.num_mixtures)\n",
        "        xhat = (im + 1.0) * 127.5\n",
        "        xhat = xhat.detach().cpu().numpy()\n",
        "        xhat = np.minimum(np.maximum(0.0, xhat), 255.0).astype(np.uint8)\n",
        "        return xhat\n",
        "\n",
        "\n",
        "def log_prob_from_logits(x):\n",
        "    \"\"\" numerically stable log_softmax implementation that prevents overflow \"\"\"\n",
        "    axis = len(x.shape) - 1\n",
        "    m = x.max(dim=axis, keepdim=True)[0]\n",
        "    return x - m - torch.log(torch.exp(x - m).sum(dim=axis, keepdim=True))\n",
        "\n",
        "\n",
        "def mpi_size():\n",
        "    return MPI.COMM_WORLD.Get_size()\n",
        "\n",
        "\n",
        "def mpi_rank():\n",
        "    return MPI.COMM_WORLD.Get_rank()\n",
        "\n",
        "\n",
        "def compute_mpi_topology():\n",
        "    world_size = mpi_size()\n",
        "    global_rank = mpi_rank()\n",
        "    # compute num_nodes\n",
        "    if world_size % 8 == 0:\n",
        "        num_nodes = world_size // 8\n",
        "    else:\n",
        "        num_nodes = world_size // 8 + 1\n",
        "    # compute gpus_per_nodes\n",
        "    if world_size > 1:\n",
        "        gpus_per_node = max(world_size // num_nodes, 1)\n",
        "    else:\n",
        "        gpus_per_node = 1\n",
        "    # local rank\n",
        "    local_rank = global_rank % gpus_per_node\n",
        "\n",
        "    return world_size, local_rank, global_rank\n",
        "\n",
        "\n",
        "def setup_mpi(H):\n",
        "    H.mpi_size, H.local_rank, H.rank = compute_mpi_topology()\n",
        "    os.environ[\"RANK\"] = str(H.rank)\n",
        "    os.environ[\"WORLD_SIZE\"] = str(H.mpi_size)\n",
        "    os.environ[\"MASTER_PORT\"] = str(H.port)\n",
        "    # os.environ[\"NCCL_LL_THRESHOLD\"] = \"0\"\n",
        "    os.environ[\"MASTER_ADDR\"] = MPI.COMM_WORLD.bcast(socket.gethostname(), root=0)\n",
        "    # 처음 한 번만 초기화\n",
        "    if not dist.is_initialized():\n",
        "      torch.cuda.set_device(H.local_rank)\n",
        "      dist.init_process_group(backend='nccl', init_method=\"env://\")   # remove f''\n",
        "\n",
        "\n",
        "def mkdir_p(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def setup_save_dirs(H):\n",
        "    H.save_dir = os.path.join(H.save_dir, H.desc)\n",
        "    mkdir_p(H.save_dir)\n",
        "    H.logdir = os.path.join(H.save_dir, 'log')\n",
        "\n",
        "\n",
        "def logger(log_prefix):\n",
        "    'Prints the arguments out to stdout, .txt, and .jsonl files'\n",
        "    jsonl_path = f'{log_prefix}.jsonl'\n",
        "    txt_path = f'{log_prefix}.txt'\n",
        "\n",
        "    def log(*args, pprint=False, **kwargs):\n",
        "        if mpi_rank() != 0:\n",
        "            return\n",
        "        t = time.ctime()\n",
        "        argdict = {'time': t}\n",
        "        if len(args) > 0:\n",
        "            argdict['message'] = ' '.join([str(x) for x in args])\n",
        "        argdict.update(kwargs)\n",
        "\n",
        "        txt_str = []\n",
        "        args_iter = sorted(argdict) if pprint else argdict\n",
        "        for k in args_iter:\n",
        "            val = argdict[k]\n",
        "            if isinstance(val, np.ndarray):\n",
        "                val = val.tolist()\n",
        "            elif isinstance(val, np.integer):\n",
        "                val = int(val)\n",
        "            elif isinstance(val, np.floating):\n",
        "                val = float(val)\n",
        "            argdict[k] = val\n",
        "            if isinstance(val, float):\n",
        "                val = f'{val:.5f}'\n",
        "            txt_str.append(f'{k}: {val}')\n",
        "        txt_str = ', '.join(txt_str)\n",
        "\n",
        "        if pprint:\n",
        "            json_str = json.dumps(argdict, sort_keys=True)\n",
        "            txt_str = json.dumps(argdict, sort_keys=True, indent=4)\n",
        "        else:\n",
        "            json_str = json.dumps(argdict)\n",
        "        print(txt_str, flush=True)\n",
        "\n",
        "        with open(txt_path, \"a+\") as f:\n",
        "            print(txt_str, file=f, flush=True)\n",
        "        with open(jsonl_path, \"a+\") as f:\n",
        "            print(json_str, file=f, flush=True)\n",
        "    return log\n",
        "\n",
        "\n",
        "def set_up_hyperparams(s=None):\n",
        "    H = Hyperparams()\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser = add_vae_arguments(parser)\n",
        "    parse_args_and_update_hparams(H, parser, s=s)\n",
        "    setup_mpi(H)\n",
        "    setup_save_dirs(H)\n",
        "    logprint = logger(H.logdir)\n",
        "    for i, k in enumerate(sorted(H)):\n",
        "        logprint(type='hparam', key=k, value=H[k])\n",
        "    np.random.seed(H.seed)\n",
        "    torch.manual_seed(H.seed)\n",
        "    torch.cuda.manual_seed(H.seed)\n",
        "    logprint('traning model', H.desc, 'on', H.dataset)\n",
        "    return H, logprint\n",
        "\n",
        "\n",
        "def linear_warmup(warmup_iters):\n",
        "    def f(iteration):\n",
        "        return iteration / warmup_iters if iteration < warmup_iters else 1.0\n",
        "    return f\n",
        "\n",
        "\n",
        "def load_vaes(encoder, decoder, image_size, logprint):\n",
        "    mpi_size, local_rank, rank = compute_mpi_topology()\n",
        "    torch.cuda.set_device(local_rank)\n",
        "\n",
        "    vae = VAE(encoder, decoder, image_size).cuda(local_rank)\n",
        "    ema_vae = VAE(encoder, decoder, image_size).cuda(local_rank)\n",
        "    ema_vae.load_state_dict(vae.state_dict())\n",
        "    ema_vae.requires_grad_(True)\n",
        "\n",
        "    if mpi_size > 1:\n",
        "        vae = DistributedDataParallel(vae, device_ids=[local_rank], output_device=local_rank)\n",
        "    # validate parameter names\n",
        "    named = list(vae.named_parameters())\n",
        "    all_params = list(vae.parameters())\n",
        "    if len(named) != len(all_params):\n",
        "        raise ValueError(\"Some parameters are unnamed-DDP requires all params to be named\")\n",
        "    total_params = 0\n",
        "    for name, p in vae.named_parameters():\n",
        "        total_params += np.prod(p.shape)\n",
        "    logprint(total_params=total_params, readable=f'{total_params:,}')\n",
        "    return vae, ema_vae\n",
        "\n",
        "\n",
        "def load_opt(H, vae, logprint):\n",
        "    optimizer = AdamW(vae.parameters(), weight_decay=H.wd, lr=H.lr, betas=(H.adam_beta1, H.adam_beta2))\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=linear_warmup(H.warmup_iters))\n",
        "\n",
        "    starting_epoch = 0\n",
        "    iterate = 0\n",
        "    cur_eval_loss = float('inf')\n",
        "    logprint('optimizer & scheduler initialized', epoch=starting_epoch, iterate=iterate, eval_loss=cur_eval_loss)\n",
        "    return optimizer, scheduler, starting_epoch, iterate, cur_eval_loss\n",
        "\n",
        "\n",
        "def allreduce(x, average):\n",
        "    if mpi_size() > 1:\n",
        "        dist.all_reduce(x, dist.ReduceOp.SUM)\n",
        "    return x / mpi_size() if average else x\n",
        "\n",
        "\n",
        "def get_cpu_stats_over_ranks(stat_dict):\n",
        "    keys = sorted(stat_dict.keys())\n",
        "    stats = torch.stack([torch.as_tensor(stat_dict[k]).detach().cuda().float() for k in keys])\n",
        "    allreduced = allreduce(stats, average=True).cpu()\n",
        "    return {k: allreduced[i].item() for (i, k) in enumerate(keys)}\n",
        "\n",
        "\n",
        "def save_model(path, vae, ema_vae, optimizer, H):\n",
        "    torch.save(vae.state_dict(), f'{path}-model.th')\n",
        "    torch.save(ema_vae.state_dict(), f'{path}-model-ema.th')\n",
        "    torch.save(optimizer.state_dict(), f'{path}-opt.th')\n",
        "    from_log = os.path.join(H.save_dir, 'log.jsonl')\n",
        "    to_log = f'{os.path.dirname(path)}/{os.path.basename(path)}-log.jsonl'\n",
        "    subprocess.check_output(['cp', from_log, to_log])\n",
        "\n",
        "\n",
        "def accumulate_stats(stats, frequency):\n",
        "    z = {}\n",
        "    for k in stats[-1]:\n",
        "        if k in ['distortion_nans', 'rate_nans', 'skipped_updates', 'gcskip']:\n",
        "            z[k] = np.sum([a[k] for a in stats[-frequency:]])\n",
        "        elif k == 'grad_norm':\n",
        "            vals = [a[k] for a in stats[-frequency:]]\n",
        "            finites = np.array(vals)[np.isfinite(vals)]\n",
        "            if len(finites) == 0:\n",
        "                z[k] = 0.0\n",
        "            else:\n",
        "                z[k] = np.max(finites)\n",
        "        elif k == 'elbo':\n",
        "            vals = [a[k] for a in stats[-frequency:]]\n",
        "            finites = np.array(vals)[np.isfinite(vals)]\n",
        "            z['elbo'] = np.mean(vals)\n",
        "            z['elbo_filtered'] = np.mean(finites)\n",
        "        elif k == 'iter_time':\n",
        "            z[k] = stats[-1][k] if len(stats) < frequency else np.mean([a[k] for a in stats[-frequency:]])\n",
        "        else:\n",
        "            z[k] = np.mean([a[k] for a in stats[-frequency:]])\n",
        "    return z\n"
      ],
      "metadata": {
        "id": "ZoU0_jUFqv55"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block.py"
      ],
      "metadata": {
        "id": "A00dFsYPq5m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch, downsample=False, residual=True, zero_last=False):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(mid_ch, mid_ch, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(mid_ch, out_ch, kernel_size=1)\n",
        "\n",
        "        if zero_last:\n",
        "            nn.init.zeros_(self.conv4.weight)\n",
        "            if self.conv4.bias is not None:\n",
        "                nn.init.zeros_(self.conv4.bias)\n",
        "\n",
        "        self.use_residual = residual\n",
        "        self.use_downsample = downsample\n",
        "        self.down = nn.AvgPool2d(kernel_size=2) if downsample else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.gelu(x)\n",
        "        out = self.conv1(out)\n",
        "        out = F.gelu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = F.gelu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = F.gelu(out)\n",
        "        out = self.conv4(out)\n",
        "\n",
        "        if self.use_residual:\n",
        "            out = out + x\n",
        "\n",
        "        out = self.down(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "3tNobjeSq8qD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder.py"
      ],
      "metadata": {
        "id": "X5QUS-EEq-e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, image_channels, base_width, custom_width_str, block_str, bottleneck_multiple):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_conv = nn.Conv2d(image_channels, base_width, kernel_size=3, padding=1)\n",
        "        self.widths = get_width_settings(base_width, custom_width_str)\n",
        "        block_config = parse_layer_string(block_str)\n",
        "\n",
        "        enc_blocks = []\n",
        "        for res, down_rate in block_config:\n",
        "            width = self.widths[res]\n",
        "            mid_width = int(width * bottleneck_multiple)\n",
        "\n",
        "            # 원본 방식: 모든 Block은 in_ch == out_ch == width\n",
        "            block = Block(\n",
        "                in_ch=width,\n",
        "                mid_ch=mid_width,\n",
        "                out_ch=width,\n",
        "                downsample=(down_rate is not None),\n",
        "                residual=True\n",
        "            )\n",
        "            enc_blocks.append(block)\n",
        "\n",
        "        self.enc_blocks = nn.ModuleList(enc_blocks)\n",
        "        self.block_resolutions = [res for res, _ in block_config]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        x = self.in_conv(x)\n",
        "\n",
        "        feats = {}\n",
        "        feats[x.shape[2]] = x  # 초기 해상도\n",
        "\n",
        "        for block, res in zip(self.enc_blocks, self.block_resolutions):\n",
        "            # 🔥 원본 방식: Block 입력 전에 채널 맞춰줌\n",
        "            if x.shape[1] != self.widths[res]:\n",
        "                x = pad_channels(x, self.widths[res])\n",
        "\n",
        "            x = block(x)\n",
        "            feats[res] = x\n",
        "\n",
        "        return feats"
      ],
      "metadata": {
        "id": "KTFLVv7YrAYE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crossattention"
      ],
      "metadata": {
        "id": "pPbzihMxu0EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim_qk, heads=4):\n",
        "        super().__init__()\n",
        "        self.dim = dim_qk\n",
        "        self.heads = heads\n",
        "        self.scale = (dim_qk // heads) ** -0.5\n",
        "\n",
        "        self.to_q = nn.Linear(dim_qk, dim_qk)\n",
        "        self.to_k = nn.Linear(dim_qk, dim_qk)\n",
        "        self.to_v = nn.Linear(dim_qk, dim_qk)\n",
        "        self.to_out = nn.Linear(dim_qk, dim_qk)\n",
        "\n",
        "    def forward(self, x_q, x_kv):\n",
        "        B, C, H, W = x_q.shape\n",
        "        N = H * W\n",
        "\n",
        "        q = self.to_q(x_q.flatten(2).permute(0, 2, 1))  # B, N, C\n",
        "        k = self.to_k(x_kv.flatten(2).permute(0, 2, 1))\n",
        "        v = self.to_v(x_kv.flatten(2).permute(0, 2, 1))\n",
        "\n",
        "        q = q.view(B, N, self.heads, C // self.heads).transpose(1, 2)  # B, heads, N, d\n",
        "        k = k.view(B, N, self.heads, C // self.heads).transpose(1, 2)\n",
        "        v = v.view(B, N, self.heads, C // self.heads).transpose(1, 2)\n",
        "\n",
        "        attn = torch.softmax(torch.matmul(q, k.transpose(-1, -2)) * self.scale, dim=-1)\n",
        "        out = torch.matmul(attn, v)  # B, heads, N, d\n",
        "        out = out.transpose(1, 2).contiguous().view(B, N, C)  # B, N, C\n",
        "        out = self.to_out(out).permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "0faukefyuz6A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DecBlock.py"
      ],
      "metadata": {
        "id": "1aBv35dVrCPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecBlock(nn.Module):\n",
        "    def __init__(self, res, width, zdim, bottleneck_multiple, mixin_res=None, n_blocks=1):\n",
        "        super().__init__()\n",
        "        self.res = res\n",
        "        self.width = width\n",
        "        self.zdim = zdim\n",
        "        self.mixin = mixin_res\n",
        "\n",
        "        cond_width = int(width * bottleneck_multiple)\n",
        "\n",
        "        self.enc = Block(width * 2, cond_width, zdim * 2, residual=False)\n",
        "        self.prior = Block(width, cond_width, zdim * 2 + width, residual=False, zero_last=True)\n",
        "\n",
        "        self.z_proj = nn.Conv2d(zdim, width, kernel_size=1)\n",
        "        self.z_proj.weight.data *= np.sqrt(1 / n_blocks)\n",
        "\n",
        "        self.resnet = Block(width, cond_width, width, residual=True)\n",
        "        self.resnet.conv4.weight.data *= np.sqrt(1 / n_blocks)\n",
        "\n",
        "        #attention\n",
        "        self.cross_attn = CrossAttention(dim_qk=width) if mixin_res is not None else None\n",
        "\n",
        "    def z_fn(self, z):\n",
        "        return self.z_proj(z)\n",
        "\n",
        "    def get_inputs(self, xs, activations):\n",
        "        acts = activations[self.res]\n",
        "        x = xs.get(self.res, torch.zeros_like(acts))\n",
        "\n",
        "        # 🔥 interpolate acts if shape mismatch\n",
        "        if acts.shape[2:] != x.shape[2:]:\n",
        "            acts = F.interpolate(acts, size=x.shape[2:], mode='nearest')\n",
        "\n",
        "        if acts.shape[0] != x.shape[0]:\n",
        "            x = x.repeat(acts.shape[0], 1, 1, 1)\n",
        "\n",
        "        return x, acts\n",
        "\n",
        "    def sample(self, x, acts):\n",
        "        qm, qv = self.enc(torch.cat([x, acts], dim=1)).chunk(2, dim=1)\n",
        "        feats = self.prior(x)\n",
        "        pm, pv, xpp = feats[:, :self.zdim], feats[:, self.zdim:self.zdim*2], feats[:, self.zdim*2:]\n",
        "        x = x + xpp\n",
        "        z = draw_gaussian_diag_samples(qm, qv)\n",
        "        kl = gaussian_analytical_kl(qm, pm, qv, pv)\n",
        "        return z, x, kl\n",
        "\n",
        "    def sample_uncond(self, x, t=None, lvs=None):\n",
        "        feats = self.prior(x)\n",
        "        pm, pv, xpp = feats[:, :self.zdim], feats[:, self.zdim:self.zdim*2], feats[:, self.zdim*2:]\n",
        "        x = x + xpp\n",
        "        if lvs is not None:\n",
        "            z = lvs\n",
        "        else:\n",
        "            if t is not None:\n",
        "                pv = pv + torch.ones_like(pv) * np.log(t)\n",
        "            z = draw_gaussian_diag_samples(pm, pv)\n",
        "        return z, x\n",
        "\n",
        "    def forward(self, xs, activations, get_latents=False):\n",
        "        x, acts = self.get_inputs(xs, activations)\n",
        "        if self.mixin is not None:\n",
        "          mix = xs[self.mixin][:, :x.shape[1], ...]\n",
        "          mix = F.interpolate(mix, size=x.shape[2:], mode='nearest')  # 해상도 맞추기\n",
        "          x = x + self.cross_attn(x, mix)\n",
        "\n",
        "        if torch.rand(1).item() < 0.01:  # 확률적으로 출력\n",
        "          print(f\"[Attn] Cross-attention applied at res {self.res} ← {self.mixin}, shape: {x.shape}\")\n",
        "\n",
        "\n",
        "        z, x, kl = self.sample(x, acts)\n",
        "        x = x + self.z_fn(z)\n",
        "        x = self.resnet(x)\n",
        "        xs[self.res] = x\n",
        "\n",
        "        if get_latents:\n",
        "            return xs, dict(z=z.detach(), kl=kl)\n",
        "        return xs, dict(kl=kl)\n",
        "\n",
        "    def forward_uncond(self, xs, t=None, lvs=None):\n",
        "        if self.res in xs:\n",
        "            x = xs[self.res]\n",
        "        else:\n",
        "            ref = xs[list(xs.keys())[0]]\n",
        "            x = torch.zeros(ref.shape[0], self.width, self.res, self.res, device=ref.device)\n",
        "\n",
        "        if self.mixin is not None:\n",
        "            mix = xs[self.mixin][:, :x.shape[1], ...]\n",
        "            mix = F.interpolate(mix, size=x.shape[2:], mode='nearest')\n",
        "            x = x + self.cross_attn(x, mix)\n",
        "\n",
        "        z, x = self.sample_uncond(x, t, lvs=lvs)\n",
        "        x = x + self.z_fn(z)\n",
        "        x = self.resnet(x)\n",
        "        xs[self.res] = x\n",
        "        return xs"
      ],
      "metadata": {
        "id": "wt6Tsws_rB6Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder.py"
      ],
      "metadata": {
        "id": "sdwof1nctUvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, res_list, width_map, zdim, bottleneck_multiple,\n",
        "                 output_res, n_blocks, num_mixtures, low_bit=False):\n",
        "        super().__init__()\n",
        "        self.output_res = output_res\n",
        "        self.width_map = width_map\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DecBlock(\n",
        "                res=res,\n",
        "                width=width_map[res],\n",
        "                zdim=zdim,\n",
        "                bottleneck_multiple=bottleneck_multiple,\n",
        "                mixin_res=mixin,\n",
        "                n_blocks=n_blocks\n",
        "            )\n",
        "            for res, mixin in res_list\n",
        "        ])\n",
        "\n",
        "        self.bias_xs = nn.ParameterDict({\n",
        "            str(res): nn.Parameter(torch.zeros(1, width_map[res], res, res))\n",
        "            for res, _ in res_list\n",
        "        })\n",
        "\n",
        "        out_width = width_map[output_res]\n",
        "        self.gain = nn.Parameter(torch.ones(1, out_width, 1, 1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1, out_width, 1, 1))\n",
        "\n",
        "        # 🔥 DmolNet 붙이기\n",
        "        self.out_net = DmolNet(width=out_width, num_mixtures=num_mixtures, low_bit=low_bit)\n",
        "\n",
        "    def final_fn(self, x):\n",
        "        return x * self.gain + self.bias\n",
        "\n",
        "    def forward(self, activations, get_latents=False):\n",
        "        B = next(iter(activations.values())).shape[0]\n",
        "        xs = {\n",
        "            int(res): bias.repeat(B, 1, 1, 1)\n",
        "            for res, bias in self.bias_xs.items()\n",
        "        }\n",
        "\n",
        "        stats = []\n",
        "        for block in self.blocks:\n",
        "            xs, block_stat = block(xs, activations, get_latents=get_latents)\n",
        "            stats.append(block_stat)\n",
        "\n",
        "        out = self.final_fn(xs[self.output_res])\n",
        "        return out, stats  # 🔥 DmolNet 통과시켜 반환\n",
        "\n",
        "    def forward_uncond(self, n, t=None):\n",
        "        xs = {\n",
        "            int(res): bias.repeat(n, 1, 1, 1)\n",
        "            for res, bias in self.bias_xs.items()\n",
        "        }\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            temp = t[idx] if isinstance(t, list) else t\n",
        "            xs = block.forward_uncond(xs, t=temp)\n",
        "\n",
        "        out = self.final_fn(xs[self.output_res])\n",
        "        return self.out_net.sample(out)  # 🔥 DmolNet 통해 샘플링\n",
        "\n",
        "    def forward_manual_latents(self, n, latents, t=None):\n",
        "        xs = {\n",
        "            int(res): bias.repeat(n, 1, 1, 1)\n",
        "            for res, bias in self.bias_xs.items()\n",
        "        }\n",
        "\n",
        "        for block, lvs in itertools.zip_longest(self.blocks, latents):\n",
        "            xs = block.forward_uncond(xs, t=t, lvs=lvs)\n",
        "\n",
        "        out = self.final_fn(xs[self.output_res])\n",
        "        return self.out_net.sample(out)\n"
      ],
      "metadata": {
        "id": "tUiK8q3LtUWS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE"
      ],
      "metadata": {
        "id": "8zcvw8q3rKIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, encoder, decoder, image_size):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.image_size = image_size  # 예: 32 (CIFAR)\n",
        "\n",
        "    def forward(self, x, x_target):\n",
        "        activations = self.encoder(x)  # 해상도별 feature map 반환\n",
        "        px_z, stats = self.decoder(activations, get_latents=True)\n",
        "\n",
        "        distortion_per_pixel = self.decoder.out_net.nll(px_z, x_target)\n",
        "        rate_per_pixel = torch.zeros_like(distortion_per_pixel)\n",
        "\n",
        "        # 각 블록에서 KL divergence를 누적\n",
        "        for stat in stats:\n",
        "            rate_per_pixel += stat['kl'].sum(dim=(1, 2, 3))\n",
        "\n",
        "        ndims = np.prod(x.shape[1:])  # 픽셀 수\n",
        "        rate_per_pixel /= ndims\n",
        "        elbo = (distortion_per_pixel + rate_per_pixel).mean()\n",
        "\n",
        "        return {\n",
        "            'elbo': elbo,\n",
        "            'distortion': distortion_per_pixel.mean(),\n",
        "            'rate': rate_per_pixel.mean()\n",
        "        }\n",
        "\n",
        "    def forward_get_latents(self, x):\n",
        "        activations = self.encoder(x)\n",
        "        _, stats = self.decoder(activations, get_latents=True)\n",
        "        return stats\n",
        "\n",
        "    def forward_uncond_samples(self, n_batch, t=None):\n",
        "        # Removed redundant call to self.decoder.out_net.sample\n",
        "        return self.decoder.forward_uncond(n_batch, t=t)\n",
        "\n",
        "    def forward_samples_set_latents(self, n_batch, latents, t=None):\n",
        "        # Removed redundant call to self.decoder.out_net.sample\n",
        "        return self.decoder.forward_manual_latents(n_batch, latents, t=t)"
      ],
      "metadata": {
        "id": "EhKPYVTUrLyM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train.py"
      ],
      "metadata": {
        "id": "DU9IY_8Dtrn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import imageio"
      ],
      "metadata": {
        "id": "HUcCoUjftsrX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xf cifar-10-python.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0Upwziptutp",
        "outputId": "a02bb2bf-a07d-42ba-ecea-5689b1798c7e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-08 13:42:26--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  13.1MB/s    in 14s     \n",
            "\n",
            "2025-06-08 13:42:42 (11.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(H, data_input, target, vae, ema_vae, optimizer, iterate):\n",
        "    t0 = time.time()\n",
        "    vae.zero_grad()\n",
        "    stats = vae.forward(data_input, target)\n",
        "\n",
        "    stats['elbo'].backward()\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(vae.parameters(), H.grad_clip).item()\n",
        "    distortion_nans = torch.isnan(stats['distortion']).sum()\n",
        "    rate_nans = torch.isnan(stats['rate']).sum()\n",
        "    stats.update(dict(rate_nans=0 if rate_nans == 0 else 1, distortion_nans=0 if distortion_nans == 0 else 1))\n",
        "    stats = get_cpu_stats_over_ranks(stats)\n",
        "\n",
        "    skipped_updates = 1\n",
        "    # only update if no rank has a nan and if the grad norm is below a specific threshold\n",
        "    if stats['distortion_nans'] == 0 and stats['rate_nans'] == 0 and (H.skip_threshold == -1 or grad_norm < H.skip_threshold):\n",
        "        optimizer.step()\n",
        "        skipped_updates = 0\n",
        "        for p1, p2 in zip(vae.parameters(), ema_vae.parameters()):\n",
        "            p2.data.mul_(H.ema_rate)\n",
        "            p2.data.add_(p1.data * (1 - H.ema_rate))\n",
        "    t1 = time.time()\n",
        "    stats.update(skipped_updates=skipped_updates, iter_time=t1 - t0, grad_norm=grad_norm)\n",
        "    return stats\n",
        "\n",
        "\n",
        "def eval_step(data_input, target, ema_vae):\n",
        "    with torch.no_grad():\n",
        "        stats = ema_vae.forward(data_input, target)\n",
        "    stats = get_cpu_stats_over_ranks(stats)\n",
        "    return stats\n",
        "\n",
        "\n",
        "def get_sample_for_visualization(data, preprocess_func, batch_size):\n",
        "    for x in DataLoader(data, batch_size=batch_size):\n",
        "        break\n",
        "    orig_image = x[0]\n",
        "    preprocessed = preprocess_func(x)[0]\n",
        "    return orig_image, preprocessed\n",
        "\n",
        "\n",
        "def train_loop(H, data_train, data_valid, preprocess_func, vae, ema_vae,\n",
        "               optimizer, scheduler, starting_epoch, iterate, cur_eval_loss, logprint):\n",
        "    train_sampler = DistributedSampler(data_train, num_replicas=H.mpi_size, rank=H.rank)\n",
        "    viz_batch_original, viz_batch_processed = get_sample_for_visualization(data_valid, preprocess_func, H.num_images_visualize) # Removed H.dataset as it's not used\n",
        "    early_evals = set([1] + [2 ** exp for exp in range(3, 14)])\n",
        "    stats = []\n",
        "    iters_since_starting = 0\n",
        "    H.ema_rate = torch.as_tensor(H.ema_rate).cuda()\n",
        "    for epoch in range(starting_epoch, H.num_epochs):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        for x in DataLoader(data_train, batch_size=H.n_batch, drop_last=True, pin_memory=True, sampler=train_sampler):\n",
        "            if H.max_iters > 0 and iterate >= H.max_iters:\n",
        "                logprint(f\"Reached max_iters={H.max_iters}, stopping training.\")\n",
        "                return\n",
        "            data_input, target = preprocess_func(x)\n",
        "            training_stats = training_step(H, data_input, target, vae, ema_vae, optimizer, iterate)\n",
        "            stats.append(training_stats)\n",
        "            scheduler.step()\n",
        "            if iterate % H.iters_per_print == 0 or iters_since_starting in early_evals:\n",
        "                logprint(model=H.desc, type='train_loss', lr=scheduler.get_last_lr()[0], epoch=epoch, step=iterate, **accumulate_stats(stats, H.iters_per_print))\n",
        "\n",
        "            if iterate % H.iters_per_images == 0 or (iters_since_starting in early_evals and H.dataset != 'ffhq_1024') and H.rank == 0:\n",
        "                write_images(H, ema_vae, viz_batch_original, viz_batch_processed, f'{H.save_dir}/samples-{iterate}.png', logprint)\n",
        "\n",
        "            iterate += 1\n",
        "            iters_since_starting += 1\n",
        "\n",
        "            # 일정 시간마다 \"latest\" 체크포인트를 저장!\n",
        "            if iterate % H.iters_per_save == 0 and H.rank == 0:\n",
        "                if np.isfinite(stats[-1]['elbo']):\n",
        "                    logprint(model=H.desc, type='train_loss', epoch=epoch, step=iterate, **accumulate_stats(stats, H.iters_per_print))\n",
        "                    fp = os.path.join(H.save_dir, 'latest')\n",
        "                    logprint(f'Saving model@ {iterate} to {fp}')\n",
        "                    save_model(fp, vae, ema_vae, optimizer, H)\n",
        "\n",
        "            if iterate % H.iters_per_ckpt == 0 and H.rank == 0:\n",
        "                save_model(os.path.join(H.save_dir, f'iter-{iterate}'), vae, ema_vae, optimizer, H)\n",
        "\n",
        "        if epoch % H.epochs_per_eval == 0:\n",
        "            valid_stats = evaluate(H, ema_vae, data_valid, preprocess_func)\n",
        "            logprint(model=H.desc, type='eval_loss', epoch=epoch, step=iterate, **valid_stats)\n",
        "\n",
        "\n",
        "def evaluate(H, ema_vae, data_valid, preprocess_func):\n",
        "    stats_valid = []\n",
        "    valid_sampler = DistributedSampler(data_valid, num_replicas=H.mpi_size, rank=H.rank)\n",
        "    for x in DataLoader(data_valid, batch_size=H.n_batch, drop_last=True, pin_memory=True, sampler=valid_sampler):\n",
        "        data_input, target = preprocess_func(x)\n",
        "        stats_valid.append(eval_step(data_input, target, ema_vae))\n",
        "    vals = [a['elbo'] for a in stats_valid]\n",
        "    finites = np.array(vals)[np.isfinite(vals)]\n",
        "    stats = dict(n_batches=len(vals), filtered_elbo=np.mean(finites), **{k: np.mean([a[k] for a in stats_valid]) for k in stats_valid[-1]})\n",
        "    return stats\n",
        "\n",
        "\n",
        "def write_images(H, ema_vae, viz_batch_original, viz_batch_processed, fname, logprint):\n",
        "    zs = [s['z'].cuda() for s in ema_vae.forward_get_latents(viz_batch_processed)]\n",
        "    batches = [viz_batch_original.numpy()]\n",
        "    mb = viz_batch_processed.shape[0]\n",
        "    lv_points = np.floor(np.linspace(0, 1, H.num_variables_visualize + 2) * len(zs)).astype(int)[1:-1]\n",
        "    for i in lv_points:\n",
        "        reconstruction = ema_vae.decoder.forward_manual_latents(mb, zs[:i], t=0.1)\n",
        "        batches.append(reconstruction)\n",
        "    for t in [1.0, 0.9, 0.8, 0.7][:H.num_temperatures_visualize]:\n",
        "        sample = ema_vae.decoder.forward_uncond(mb, t=t)\n",
        "        batches.append(sample)\n",
        "    n_rows = len(batches)\n",
        "    im = np.concatenate(batches, axis=0).reshape((n_rows, mb, *viz_batch_processed.shape[1:])).transpose([0, 2, 1, 3, 4]).reshape([n_rows * viz_batch_processed.shape[1], mb * viz_batch_processed.shape[2], 3]).astype(np.uint8) # Explicitly cast to uint8\n",
        "    logprint(f'printing samples to {fname}')\n",
        "    imageio.imwrite(fname, im)\n",
        "\n",
        "\n",
        "def run_test_eval(H, ema_vae, data_test, preprocess_func, logprint):\n",
        "    print('evaluating')\n",
        "    stats = evaluate(H, ema_vae, data_test, preprocess_func)\n",
        "    print('test results')\n",
        "    for k in stats:\n",
        "        print(k, stats[k])\n",
        "    logprint(type='test_loss', **stats)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Encoder/Decoder/VAE 생성\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    encoder = Encoder(\n",
        "        image_channels=image_channels,\n",
        "        base_width=base_width,\n",
        "        custom_width_str=custom_width_str,\n",
        "        block_str=block_str,\n",
        "        bottleneck_multiple=bottleneck_multiple\n",
        "    )\n",
        "    decoder = Decoder(\n",
        "        res_list=res_list,\n",
        "        width_map=encoder.widths,\n",
        "        zdim=zdim,\n",
        "        bottleneck_multiple=bottleneck_multiple,\n",
        "        output_res=image_size,\n",
        "        n_blocks=n_blocks,\n",
        "        num_mixtures=num_mixtures,\n",
        "        low_bit=False\n",
        "    )\n",
        "\n",
        "    H, logprint = set_up_hyperparams(s=[])\n",
        "    H.device = device\n",
        "    H, data_train, data_valid_or_test, preprocess_func = set_up_data(H)\n",
        "    vae, ema_vae = load_vaes(encoder, decoder, image_size, logprint)\n",
        "\n",
        "    # 저장된 checkpoint에서 학습을 다시 시작하는 경우\n",
        "    # load vae parameter\n",
        "    if H.restore_path is not None:\n",
        "        model_ckpt = f\"{H.restore_path}-model.th\"\n",
        "        ckpt_vae = torch.load(model_ckpt, map_location=H.device)\n",
        "        vae.load_state_dict(ckpt_vae)\n",
        "        vae = vae.to(device)\n",
        "        # load ema_vae parameter\n",
        "        if H.restore_ema_path is not None:\n",
        "            ema_ckpt = f\"{H.restore_ema_path}-model-ema.th\"\n",
        "            ckpt_ema = torch.load(ema_ckpt, map_location=H.device)\n",
        "            ema_vae.load_state_dict(ckpt_ema)\n",
        "            ema_vae = ema_vae.to(H.device)\n",
        "        print(f\">> Loaded model & EMA from {H.restore_path}.\")\n",
        "    else:\n",
        "        vae = vae.to(device)\n",
        "        ema_vae = ema_vae.to(device)\n",
        "\n",
        "    # generate optimizer & load optimizer\n",
        "    optimizer, scheduler, starting_epoch, iterate, cur_eval_loss = load_opt(H, vae, logprint)\n",
        "    if H.restore_optimizer_path is not None:\n",
        "        optimizer_ckpt = torch.load(H.restore_optimizer_path, map_location=H.device)\n",
        "        optimizer.load_state_dict(optimizer_ckpt)\n",
        "        print(f\">> Loaded optimizer state from {H.restore_optimizer_path}\")\n",
        "\n",
        "    # 실제 evaluation & test loop\n",
        "    if H.test_eval:\n",
        "        run_test_eval(H, ema_vae, data_valid_or_test, preprocess_func, logprint)\n",
        "    else:\n",
        "        train_loop(H, data_train, data_valid_or_test, preprocess_func, vae, ema_vae,\n",
        "                   optimizer, scheduler, starting_epoch, iterate, cur_eval_loss, logprint)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAMZ9dm2twff",
        "outputId": "b8c15c01-800a-4f71-e1e0-ac72c9097226"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: adam_beta1, value: 0.90000\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: adam_beta2, value: 0.90000\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: data_root, value: ../content\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: dataset, value: cifar10\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: desc, value: test\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: ema_rate, value: 0.99980\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: epochs_per_eval, value: 10\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: epochs_per_eval_save, value: 20\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: epochs_per_probe, value: None\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: grad_clip, value: 200.00000\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: iters_per_ckpt, value: 25000\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: iters_per_images, value: 10000\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: iters_per_print, value: 1000\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: iters_per_save, value: 1500\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: local_rank, value: 0\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: logdir, value: ./saved_models/test/log\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: lr, value: 0.00020\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: lr_prior, value: 0.00015\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: max_iters, value: 1563\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: mpi_size, value: 1\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: n_batch, value: 32\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: num_epochs, value: 10\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: num_images_visualize, value: 8\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: num_temperatures_visualize, value: 3\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: num_variables_visualize, value: 6\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: port, value: 29500\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: rank, value: 0\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: restore_ema_path, value: None\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: restore_log_path, value: None\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: restore_optimizer_path, value: None\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: restore_path, value: None\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: save_dir, value: ./saved_models/test\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: seed, value: 0\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: skip_threshold, value: 400.00000\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: temperature, value: 1.00000\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: test_eval, value: False\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: warmup_iters, value: 100\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: wd, value: 0.01000\n",
            "time: Sun Jun  8 13:43:18 2025, type: hparam, key: wd_prior, value: 0.00000\n",
            "time: Sun Jun  8 13:43:18 2025, message: traning model test on cifar10\n",
            "time: Sun Jun  8 13:43:19 2025, total_params: 14533668, readable: 14,533,668\n",
            "time: Sun Jun  8 13:43:24 2025, message: optimizer & scheduler initialized, epoch: 0, iterate: 0, eval_loss: inf\n",
            "time: Sun Jun  8 13:43:28 2025, model: test, type: train_loss, lr: 0.00000, epoch: 0, step: 0, distortion: 6.20155, distortion_nans: 0.00000, elbo: 6.24294, elbo_filtered: 6.24294, rate: 0.04139, rate_nans: 0.00000, skipped_updates: 0, iter_time: 4.11305, grad_norm: 0.72670\n",
            "time: Sun Jun  8 13:43:29 2025, message: printing samples to ./saved_models/test/samples-0.png\n",
            "time: Sun Jun  8 13:43:30 2025, model: test, type: train_loss, lr: 0.00000, epoch: 0, step: 1, distortion: 6.26767, distortion_nans: 0.00000, elbo: 6.30908, elbo_filtered: 6.30908, rate: 0.04141, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.43475, grad_norm: 0.74059\n",
            "time: Sun Jun  8 13:43:30 2025, message: printing samples to ./saved_models/test/samples-1.png\n",
            "time: Sun Jun  8 13:43:33 2025, model: test, type: train_loss, lr: 0.00002, epoch: 0, step: 8, distortion: 6.27207, distortion_nans: 0.00000, elbo: 6.31316, elbo_filtered: 6.31316, rate: 0.04109, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.45526, grad_norm: 0.75669\n",
            "time: Sun Jun  8 13:43:34 2025, message: printing samples to ./saved_models/test/samples-8.png\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "time: Sun Jun  8 13:43:37 2025, model: test, type: train_loss, lr: 0.00003, epoch: 0, step: 16, distortion: 6.25884, distortion_nans: 0.00000, elbo: 6.29884, elbo_filtered: 6.29884, rate: 0.04000, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.44234, grad_norm: 0.75669\n",
            "time: Sun Jun  8 13:43:37 2025, message: printing samples to ./saved_models/test/samples-16.png\n",
            "time: Sun Jun  8 13:43:45 2025, model: test, type: train_loss, lr: 0.00007, epoch: 0, step: 32, distortion: 6.20134, distortion_nans: 0.00000, elbo: 6.23666, elbo_filtered: 6.23666, rate: 0.03533, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.44326, grad_norm: 0.90375\n",
            "time: Sun Jun  8 13:43:45 2025, message: printing samples to ./saved_models/test/samples-32.png\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "time: Sun Jun  8 13:43:59 2025, model: test, type: train_loss, lr: 0.00013, epoch: 0, step: 64, distortion: 5.80449, distortion_nans: 0.00000, elbo: 5.82703, elbo_filtered: 5.82703, rate: 0.02254, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.44807, grad_norm: 14.71238\n",
            "time: Sun Jun  8 13:44:00 2025, message: printing samples to ./saved_models/test/samples-64.png\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "time: Sun Jun  8 13:44:29 2025, model: test, type: train_loss, lr: 0.00020, epoch: 0, step: 128, distortion: 5.33524, distortion_nans: 0.00000, elbo: 5.35261, elbo_filtered: 5.35261, rate: 0.01737, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.45571, grad_norm: 23.65516\n",
            "time: Sun Jun  8 13:44:29 2025, message: printing samples to ./saved_models/test/samples-128.png\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "time: Sun Jun  8 13:45:29 2025, model: test, type: train_loss, lr: 0.00020, epoch: 0, step: 256, distortion: 5.01211, distortion_nans: 0.00000, elbo: 5.02935, elbo_filtered: 5.02935, rate: 0.01724, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.47235, grad_norm: 26.06866\n",
            "time: Sun Jun  8 13:45:29 2025, message: printing samples to ./saved_models/test/samples-256.png\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "time: Sun Jun  8 13:47:33 2025, model: test, type: train_loss, lr: 0.00020, epoch: 0, step: 512, distortion: 4.71589, distortion_nans: 0.00000, elbo: 4.74954, elbo_filtered: 4.74954, rate: 0.03366, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.47374, grad_norm: 26.06866\n",
            "time: Sun Jun  8 13:47:33 2025, message: printing samples to ./saved_models/test/samples-512.png\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "time: Sun Jun  8 13:51:29 2025, model: test, type: train_loss, lr: 0.00020, epoch: 0, step: 1000, distortion: 4.43958, distortion_nans: 0.00000, elbo: 4.51294, elbo_filtered: 4.51294, rate: 0.07336, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.47637, grad_norm: 26.06866\n",
            "time: Sun Jun  8 13:51:40 2025, model: test, type: train_loss, lr: 0.00020, epoch: 0, step: 1024, distortion: 4.38807, distortion_nans: 0.00000, elbo: 4.46373, elbo_filtered: 4.46373, rate: 0.07566, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.47726, grad_norm: 26.06866\n",
            "time: Sun Jun  8 13:51:41 2025, message: printing samples to ./saved_models/test/samples-1024.png\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 8 ← 4, shape: torch.Size([32, 384, 8, 8])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "time: Sun Jun  8 13:55:10 2025, model: test, type: eval_loss, epoch: 0, step: 1406, n_batches: 156, filtered_elbo: 4.13758, distortion: 3.98827, elbo: 4.13758, rate: 0.14931\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 4 ← 1, shape: torch.Size([32, 384, 4, 4])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "time: Sun Jun  8 13:55:55 2025, model: test, type: train_loss, epoch: 1, step: 1500, distortion: 4.08883, distortion_nans: 0.00000, elbo: 4.21916, elbo_filtered: 4.21916, rate: 0.13033, rate_nans: 0.00000, skipped_updates: 0, iter_time: 0.48178, grad_norm: 15.00970\n",
            "time: Sun Jun  8 13:55:55 2025, message: Saving model@ 1500 to ./saved_models/test/latest\n",
            "[Attn] Cross-attention applied at res 1 ← None, shape: torch.Size([32, 384, 1, 1])\n",
            "[Attn] Cross-attention applied at res 32 ← 16, shape: torch.Size([32, 384, 32, 32])\n",
            "[Attn] Cross-attention applied at res 16 ← 8, shape: torch.Size([32, 384, 16, 16])\n",
            "time: Sun Jun  8 13:56:26 2025, message: Reached max_iters=1563, stopping training.\n"
          ]
        }
      ]
    }
  ]
}